{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# DAV 6150 Module 4: Dimensionality Reduction & Feature Selection\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4\n",
    "\n",
    "\n",
    "## Machine Learning Models: \"Simpler\" is Better\n",
    "\n",
    "- Machine learning models that are overly complex can be difficult, if not impossible, to interpret / explain\n",
    "\n",
    "\n",
    "- Complex models can also be very computationally expensive\n",
    "\n",
    "\n",
    "- A great deal of model complexity can be eliminated by simply reducing the number of explanatory variables employed within a model.\n",
    "\n",
    "\n",
    "- Models can be simplified through the use of __Dimensionality Reduction__ and __Feature Selection__ techniques.\n",
    "\n",
    "\n",
    "## Dimensionality Reduction\n",
    "\n",
    "__What is \"Dimensionality\"?__: Within the context of data science and machine learning, \"dimensionality\" refers to the number of potential explanatory variables (a.k.a., \"features\") contained within a set of data we plan to make use of for the construction of a model.\n",
    "\n",
    "\n",
    "__What is \"The Curse of Dimensionality\"?__: If the number of features available to us within a data set is very large relative to the number of observations, some machine learning algorithms will fail to produce effective models, particularly those which are based on similarity metrics (e.g., K-nearest neighbors, clustering, etc.). \n",
    "\n",
    "\n",
    "__Dimensionality Reduction__: We can reduce the number of __numeric__ features to be used within a model via the application of linear algebra techniques (e.g., Principal Components Analysis (PCA); Singular Value Decomposition (SVD)). These methods allow us to derive orthogonal features from a non-orthogonal collection of __numeric__ features. We can then use these new orthogonal features as explanatory variables within our machine learning models. \n",
    "\n",
    "\n",
    "### Principal Components Analysis (PCA)\n",
    "\n",
    "- PCA uses an orthogonal transformation to convert a set of possibly correlated __numeric__ features into a set of values of linearly uncorrelated (a.k.a., __orthogonal__) features known as __principal components__. These new features are then used in a machine learning model __in place of the original features that were used to generate the principal components__.  The principal components themselves are assessed by ranking them in order of the amount of variance in the data they explain. When used within a machine learning context, we generally opt to select the most relevant principal components (i.e., those that explain the greatest amount of variance). For example, we might decide we want to retain the principal components that cumulatively explain at least x% of the variability in the data. (__NOTE__: it is __NOT__ appropriate to use PCA on categorical features, even when they have been converted to binary \"one-hot\" dummy variables. If you want to reduce the dimensionality of categorical data you should instead rely on the use of __feature selection__ techniques).\n",
    "\n",
    "\n",
    "- Unfortunately, there is no strict rule of thumb to apply when deciding how many principal components to retain for use within a machine learning model. \n",
    "\n",
    "\n",
    "- __A major benefit of PCA__: PCA can often drastically the dimensionality of our data, thereby greatly reducing the number of explanatory variables needed to be included within a machine learning model.\n",
    "\n",
    "\n",
    "- __A major drawback to PCA__: Principal components are extremely difficult to interpret / explain.  As such, any model constructed with principal components will also be __very__ difficult to interpret / explain. \n",
    "\n",
    "\n",
    "- __Another major drawback to PCA__: PCA should not be applied to categorical features, even if they have been converted to binary \"one-hot\" dummy variables.\n",
    "\n",
    "\n",
    "- A good, simple example from your assigned readings: https://towardsdatascience.com/pca-and-svd-explained-with-numpy-5d13b0d2a4d8\n",
    "\n",
    "\n",
    "- Within Python we can make use of the __sklearn.decomposition.PCA()__ function to identify the principal components of a data set: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "\n",
    "\n",
    "Now let's look at a simple example of how to use the output of PCA as input to a machine learning algorithm (adapted from https://stackoverflow.com/questions/32194967/how-to-do-pca-and-svm-for-classification-in-python): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load numpy + 'datasets'. We'll use the \"iris\" data that is provided with sklearn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# load PCA + SVM classifier (\"SVC\") + cross validation functions\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import model_selection\n",
    "\n",
    "# load the iris dataset\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# what is the dimensionality of the data?\n",
    "iris.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, we have 150 observations comprised of 4 features. What do the features look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.1, 3.5, 1.4, 0.2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the first row of the array\n",
    "iris.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, each of the features contains floating point numbers. Since all of the features are numeric we can apply PCA to the data set in an attempt to reduce its dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9348581  0.04635375]\n"
     ]
    }
   ],
   "source": [
    "# assign the explanatory variables to a Python object\n",
    "X = iris.data\n",
    "\n",
    "# assign the response variable to a Python object\n",
    "y = iris.target\n",
    "\n",
    "# split the data into training + testing subsets\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.4, random_state=0)\n",
    "\n",
    "# create an instance of a PCA model +\n",
    "# set the number of components you want to retain\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# apply the PCA function to the training data\n",
    "pca.fit(X_train)\n",
    "\n",
    "# display the explained variance ratio for the principal components we've derived from the data\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The application of PCA to the data set has yielded two principal components, the first of which explains 93.485% of the variance in the iris data. The second principal component explains an additional 4.635% of the variance in the data. Therefore, the two principal components explain a total of more than 98% of the variance in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20.62747613  4.59320324]\n"
     ]
    }
   ],
   "source": [
    "# display the singular values associated with the 2 principal components\n",
    "print(pca.singular_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "# now apply the results of the PCA to the training data to transform it into 2 principal components per observation\n",
    "X_t_train = pca.transform(X_train)\n",
    "\n",
    "# apply the results of the PCA to the testing data to transform it into 2 principal components per observation\n",
    "X_t_test = pca.transform(X_test)\n",
    "\n",
    "# create an instance of an SVM classifier\n",
    "clf = SVC()\n",
    "\n",
    "# fit the SVM classifier to the transformed training data + the response data\n",
    "clf.fit(X_t_train, y_train)\n",
    "\n",
    "# check the accuracy of the SVM classifier using the transformed explanatory variables + the response variable\n",
    "print ('score', clf.score(X_t_test, y_test) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our SVM classifier has achieved an accuracy score of 93.3% when applied to the two principal components we derived from the iris data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred label [2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 1 0 0 1 1 0 2 1 0 1 2 1 0\n",
      " 2 1 1 2 0 2 0 0 1 2 2 2 2 1 2 1 1 2 2 1 2 1 2]\n"
     ]
    }
   ],
   "source": [
    "# if desired, generate predictions from the SVM classifier for the transformed testing data\n",
    "print ('pred label', clf.predict(X_t_test) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "We can determine which attributes/features to include within a model via the application of a variety of thresholding \"filters\" (e.g., exclude all variables whose variance falls below a certain value; if two or more variables are highly correlated with one another, choose one to use within the model and exclude the others; etc.)\n",
    "\n",
    "Some of the most commonly used feature selection techniques include:\n",
    "\n",
    "__Variance Threshholds__: Start by normalizing the features you plan to use as explanatory variables, then calculate their variances. Features whose values show relatively little variance are much less likely to be introducing valuable information within the context of a model, so they are strong candidates for exclusion from your model. How you choose the threshold value is highly subjective / empirical.\n",
    "\n",
    "\n",
    "__Correlation Thresholds__: Remove features that are highly correlated with other features. As with variance thresholds, the choice of a correlation value to use as a threshold is highly subjectve / empirical.\n",
    "\n",
    "\n",
    "__Forward Selection__: Used in regression modeling. Incrementally add features to a model one at a time until model performance no longer improves. A common approach to Forward Stepwise Search is to begin your modeling using the explanatory variable that is most highly correlated with the response variable, then sequentially add additional explanatory variables in decreasing order of their correlation with the response variable. The general algorithm is as follows:\n",
    "\n",
    "- Start with the null model, a model containing an intercept but no predictors.\n",
    "- Fit a simple linear regression model to each individual explanatory variable and then add to the null model that variable resulting in the lowest residual sum of squares value (\"RSS\").\n",
    "- Add to that model the variable that results in the lowest RSS amongst all two-variable models.\n",
    "- The algorithm continues until some stopping rule is satisfied (i.e. all remaining variables have a p-value greater than some\n",
    "threshold).\n",
    "\n",
    "An example of one way to implement forward selection in Python can be found here: https://towardsdatascience.com/feature-selection-using-wrapper-methods-in-python-f0d352b346f\n",
    "\n",
    "\n",
    "__Backward Selection__: Used in regression modeling. Start your modeling process using all of the explanatory variables you believe to be appropriate, then sequentially remove variables one at a time until model performance starts to substantively degrade. The general algorithm is as follows:\n",
    "\n",
    "- Begin with all variables in the model.\n",
    "- Remove the variable with the largest p-value (i.e. least statistically significant).\n",
    "- The new model is fit, and the variable with the largest p-value is removed.\n",
    "- The algorithm continues until a stopping rule is reached (e.g., the p-values of all variables are <= 0.05)\n",
    "\n",
    "An example of one way to implement backward selection in Python can be found here: https://towardsdatascience.com/feature-selection-using-wrapper-methods-in-python-f0d352b346f\n",
    "\n",
    "\n",
    "__Variance Inflation Factors (VIF)__: A regression-specific metric that can use to help us select features for inclusion within a regression algorithm. A VIF value provides an indication of the presence of multicolinearity amongst the explanatory variables used for regression modeling. VIF is calculated by regressing an explanatory variable against every other available explanatory variable. In general, if your VIF calculations for a feature result in a VIF > 5, that feature is a strong candidate for removal from the model: rerun the model without the variable and check to see whether the model's performance has improved. If so, exclude the feature from the model. If not, leave the feature in the model. An excellent overview of VIF is provided in your assigned reading materials: https://www.statisticshowto.datasciencecentral.com/variance-inflation-factor/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4 Assignment Guidelines / Requirements\n",
    "\n",
    "\n",
    "# Final Project Guidelines / Requirements"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
